{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fab7f2",
   "metadata": {},
   "source": [
    "# Analyzing Residential Properties in Ames, Iowa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fecc8f",
   "metadata": {},
   "source": [
    "## Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b1a7d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_log_error\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Flatten\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/engine/functional.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from xgboost import XGBRegressor\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train = 'https://raw.githubusercontent.com/ss16318/AML_Group_3/main/train.csv'\n",
    "df_train_raw = pd.read_csv(train)\n",
    "\n",
    "test_x = 'https://raw.githubusercontent.com/ss16318/AML_Group_3/main/test.csv'\n",
    "df_test_x_raw = pd.read_csv(test_x)\n",
    "\n",
    "test_y = 'https://raw.githubusercontent.com/ss16318/AML_Group_3/main/sample_submission.csv'\n",
    "df_test_y_raw= pd.read_csv(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original dataset on the Kaggle is divided into train and test set, we combine two dataset and split afterward in our way.\n",
    "# Merge train dataset and test dataset\n",
    "df_test_raw = pd.merge(df_test_x_raw, df_test_y_raw, on='Id')\n",
    "df_raw = pd.concat([df_train_raw,df_test_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da0481",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41de441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data description, when these values are nan, the property does not contain installment. \n",
    "# So we will change these features from nan to None\n",
    "na_none = ['Alley','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', \n",
    "     'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of missing data for each feature in train dataset\n",
    "na_col = [i for i in df_raw.columns if df_raw[i].isnull().sum() > 0 and i not in na_none]\n",
    "print((df_raw[na_col].isna().sum()/len(df_raw.Id)).sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ad1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # Replace the missing value to None if feature is in the na_none list\n",
    "        if col in na_none:\n",
    "            df[col] =  df[col].fillna('None')\n",
    "        \n",
    "        # Drop features that contain more than 20% missing values\n",
    "        elif df[col].isnull().sum() / df.shape[0] > 0.1:\n",
    "            df = df.drop(col, axis=1)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ed2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = clean_df(df_raw) # drop columns and replace nan with none\n",
    "\n",
    "df_cleaned = df_cleaned.dropna() #drop any row with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop ID column\n",
    "df_cleaned = df_cleaned.drop('Id', axis=1)\n",
    "\n",
    "# split train dataset\n",
    "df_x = df_cleaned.drop(columns='SalePrice')\n",
    "df_y = df_cleaned['SalePrice']\n",
    "\n",
    "df_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e05075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206487e4",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e59fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.nunique(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098baec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61da0e3",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "    'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
    "    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "    'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "    'PavedDrive', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n",
    "    'SaleCondition'\n",
    "]\n",
    "\n",
    "numerical = [\n",
    "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
    "    'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
    "    'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n",
    "    'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n",
    "    'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold',\n",
    "]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "sns.set(font_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3850db",
   "metadata": {},
   "source": [
    "#### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 6, figsize=(50, 50))\n",
    "for variable, subplot in zip(categorical, ax.flatten()):\n",
    "    sns.barplot(x = variable, y = \"SalePrice\", data=df_cleaned, palette='rainbow', ax=subplot).set(title=f\"{variable} Bar Chart\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_categorical = [\n",
    "    'Neighborhood', 'CentralAir', 'Heating', 'HeatingQC', 'Electrical', 'MSZoning', 'LotConfig', 'LandSlope', 'RoofStyle'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(30, 30))\n",
    "for variable, subplot in zip(specific_categorical, ax.flatten()):\n",
    "    sns.barplot(x = variable, y = \"SalePrice\", data=df_cleaned, palette='rainbow', ax=subplot).set(title=f\"{variable} Bar Chart\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f593a2",
   "metadata": {},
   "source": [
    "#### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 6, figsize=(100, 100))\n",
    "for variable, subplot in zip(numerical, ax.flatten()):\n",
    "    sns.histplot(x = variable, data=df_x, ax=subplot).set(title=f\"{variable} Distribution\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_numerical = [\n",
    "    'LotArea', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageArea', 'MoSold', 'YearBuilt'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(30, 30))\n",
    "for variable, subplot in zip(specific_numerical, ax.flatten()):\n",
    "    sns.histplot(x = variable, data=df_x, ax=subplot).set(title=f\"{variable} Distribution\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f31258",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ba6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 6, figsize=(50, 50))\n",
    "for variable, subplot in zip(numerical, ax.flatten()):\n",
    "    sns.scatterplot(x=variable, y=\"SalePrice\", data=df_cleaned, ax=subplot).set(title=f\"{variable} vs Sale Price\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637828d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_bivariate_numerical = [\n",
    "    'MoSold', 'YrSold', 'BedroomAbvGr', 'OverallQual', 'YearBuilt', 'GarageArea', 'GrLivArea', 'TotalBsmtSF'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(30, 30))\n",
    "for variable, subplot in zip(specific_bivariate_numerical, ax.flatten()):\n",
    "    sns.scatterplot(x=variable, y=\"SalePrice\", data=df_cleaned, ax=subplot).set(title=f\"{variable} vs Sale Price\")\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f264eda",
   "metadata": {},
   "source": [
    "#### Y-test summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae491b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a small multiple of bar charts\n",
    "fig, ax = plt.subplots(1, 6, figsize=(12, 4))\n",
    "for i, var in enumerate(['Street', 'MSSubClass', 'HouseStyle', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']):\n",
    "    df_cleaned[var].value_counts().plot(kind='bar', ax=ax[i])\n",
    "    ax[i].set_title(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7291a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume 'SalePrice' is the target variable in your dataframe\n",
    "y = df_cleaned['SalePrice']\n",
    "\n",
    "# create a histogram of the target variable\n",
    "plt.hist(y, bins=50)\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating y-test summary stats\n",
    "mean = np.mean(df_y)\n",
    "median = np.median(df_y)\n",
    "std_dev = np.std(df_y)\n",
    "skewness = df_y.skew()\n",
    "kurtosis = df_y.kurtosis()\n",
    "\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Median: \", median)\n",
    "print(\"Standard Deviation: \", std_dev)\n",
    "print(\"Skewness: \", skewness)\n",
    "print(\"Kurtosis: \", kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c990c",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2af3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List categorical features\n",
    "categorical_feats = df_x.select_dtypes('object').columns\n",
    "categorical_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260958eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode variables\n",
    "\n",
    "# Target Encode features that have many groups / are not ordinal\n",
    "target_cat_feats = ['GarageType', 'Functional', 'Electrical', 'SaleCondition', 'SaleType', 'Heating', 'BsmtFinType2', 'BsmtFinType1', 'Foundation', 'MSZoning', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType']\n",
    "target_encoder = ce.TargetEncoder(cols=target_cat_feats)\n",
    "\n",
    "target_encoder.fit(df_x, df_y)\n",
    "df_x_te = target_encoder.transform(df_x)\n",
    "\n",
    "# Ordinal Encode features with few groups / have a natural order\n",
    "ordinal_cat_feats = ['FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature', 'Alley', 'KitchenQual', 'PavedDrive', 'GarageCond', 'GarageQual', 'GarageFinish', 'CentralAir', 'HeatingQC', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond']\n",
    "ordinal_encoder = ce.OrdinalEncoder(cols=ordinal_cat_feats)\n",
    "\n",
    "ordinal_encoder.fit(df_x_te, df_y)\n",
    "df_x_encoded = ordinal_encoder.transform(df_x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will probably to PCA, so looking at correlations is probably not too important\n",
    "\n",
    "#plot correlation matrix\n",
    "plt.figure(figsize=(100,100))\n",
    "corr_matrix = df_x_encoded.corr()\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Take absolute values of correlations and select upper triangle of correlation matrix\n",
    "upper_matrix = corr_matrix.abs().where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "correlated_features = [column for column in upper_matrix.columns if any(upper_matrix[column] > 0.9)]\n",
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb446ebc",
   "metadata": {},
   "source": [
    "## Sort, Split and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff227159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Data Chronologically (preventing data leakage)\n",
    "df_x_encoded = df_x_encoded.reset_index(drop=True)\n",
    "df_y = df_y.reset_index(drop=True)\n",
    "\n",
    "# Sort df_x_encoded by YearSold\n",
    "df_x_sorted = df_x_encoded.sort_values(by='YrSold')\n",
    "\n",
    "# Set the index of df_y to match the sorted index of df_x_sorted\n",
    "df_y_sorted = df_y.loc[df_x_sorted.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Dev and Test\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(df_x_sorted, df_y_sorted, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split Dev into Train and Test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scale = StandardScaler()\n",
    "X_train_scaled = scale.fit_transform(X_train)\n",
    "X_val_scaled = scale.transform(X_val)\n",
    "X_test_scaled = scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d4a28",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm.fit(df_x_scaled)\n",
    "\n",
    "# Predict the labels for the data\n",
    "labels = gmm.predict(df_x_scaled)\n",
    "\n",
    "# Create a list of y values for each cluster\n",
    "y_cluster = [df_y[labels == i] for i in range(gmm.n_components)]\n",
    "\n",
    "# Sort clusters in ascending order (by median)\n",
    "sorted_y_cluster = sorted(y_cluster, key=lambda x: np.median(x))\n",
    "\n",
    "# Boxplot of clusters\n",
    "plt.boxplot(sorted_y_cluster)\n",
    "plt.title(\"Boxplots of House Prices by Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.show()\n",
    "\n",
    "# Count number of points in each cluster\n",
    "for i in range(0,len(sorted_y_cluster)):\n",
    "    print(\"Cluster \" + str(i+1) + \": \" + str(len(sorted_y_cluster[i])) + \" data points\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc896765",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Apply PCA to scaled data and compute the variance ratio:\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "#Plot the explained variance ratio as a function of the number of principal components:\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), label='Explained Variance')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.hlines(y=0.8, xmin=0, xmax=80, linestyles='dashed', colors='red', label='CEV Threshold')\n",
    "plt.vlines(x=36, ymin=0, ymax=1, linestyles='dashed', colors='green', label='Component Cut-off')\n",
    "plt.xlim((0,80))\n",
    "plt.ylim((0,1.05))\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Explained Variance vs. Number of Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the number of PCA that explain a sufficient proportion of the variance\n",
    "n_components = np.where(np.cumsum(explained_variance_ratio) >= 0.80)[0][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA with the optimal number of principal components:\n",
    "pca_optimal = PCA(n_components=n_components)\n",
    "\n",
    "X_train_pca = pca_optimal.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca_optimal.fit_transform(X_val_scaled)\n",
    "X_test_pca = pca_optimal.fit_transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with principal components and columns named 'PC1', 'PC2', etc.\n",
    "df_principal_components = pd.DataFrame(X_train_pca, columns=['PC' + str(i + 1) for i in range(n_components)])\n",
    "df_principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d2865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Important Features for PCA\n",
    "\n",
    "#Get eigenvectors and eigenvalues\n",
    "eigenvectors = pca.components_[:, :n_components]\n",
    "eigenvalues = pca.explained_variance_[:n_components].reshape(-1, 1)\n",
    "\n",
    "#Calculate feature importance\n",
    "feature_imp = np.zeros((eigenvectors.shape[0]))\n",
    "for i in range(0,len(eigenvalues)):\n",
    "    feature_imp += np.abs(eigenvalues[i] * eigenvectors[:,i])\n",
    "\n",
    "# Get the indices that would sort the feature_imp in ascending order\n",
    "sort_ind = np.argsort(feature_imp)\n",
    "\n",
    "# Select the last 10 indices (corresponding to the largest elements)\n",
    "largest_feat_ind = sort_ind[-10:]\n",
    "largest_feat_ind = largest_feat_ind[::-1]  # Reverse order to get largest to smallest\n",
    "\n",
    "# Get the top 10 elements and their indices\n",
    "most_imp = feature_imp[largest_feat_ind]\n",
    "\n",
    "# Get a list of column names corresponding to the selected features\n",
    "feat_names = list(df_x_encoded.columns[largest_feat_ind])\n",
    "\n",
    "# Create the bar plot\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.bar(range(len(most_imp)), most_imp)\n",
    "plt.xticks(range(len(feat_names)), feat_names)\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Important Features for PCA')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8d6c7",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fe261",
   "metadata": {},
   "source": [
    "### Linear regression on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "print(\"RMSLE: \", np.sqrt(mean_squared_log_error(y_test, linear.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "linear_coefs = linear.coef_\n",
    "names = df_x.columns.to_list()\n",
    "ax = sns.barplot(x=names, y=linear_coefs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6375e",
   "metadata": {},
   "source": [
    "### LASSO regression on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=0)\n",
    "lasso_alpha = {'alpha': np.logspace(-5, 2, 20)}\n",
    "grid_search_lasso = GridSearchCV(lasso, lasso_alpha)\n",
    "grid_search_lasso.fit(X_val, y_val)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search_lasso.best_params_)\n",
    "\n",
    "best_lasso = Lasso(alpha=grid_search_lasso.best_params_['alpha'], random_state=0)\n",
    "best_lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"RMSLE: \", np.sqrt(mean_squared_log_error(y_test, best_lasso.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig = plt.figure(figsize= (16,10))\n",
    "lasso_coefs = best_lasso.coef_  \n",
    "ax = sns.barplot(x=names, y=lasso_coefs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253dadd9",
   "metadata": {},
   "source": [
    "### Linear regression using Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pca = LinearRegression()\n",
    "linear_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"RMSLE: \", np.sqrt(mean_squared_log_error(y_test, linear_pca.predict(X_test_pca))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig = plt.figure(figsize= (16,8))\n",
    "linear_coefs_pca = linear_pca.coef_\n",
    "names = df_principal_components.columns.to_list()\n",
    "ax = sns.barplot(x=names, y=linear_coefs_pca)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f630d78",
   "metadata": {},
   "source": [
    "### LASSO regression using Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lasso_pca = GridSearchCV(lasso, lasso_alpha)\n",
    "grid_search_lasso_pca.fit(X_val_pca, y_val)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search_lasso_pca.best_params_)\n",
    "\n",
    "best_lasso_pca = Lasso(alpha=grid_search_lasso_pca.best_params_['alpha'], random_state=0)\n",
    "best_lasso_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"RMSLE: \", np.sqrt(mean_squared_log_error(y_test, best_lasso_pca.predict(X_test_pca))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "fig = plt.figure(figsize= (16,8))\n",
    "lasso_coefs_pca = best_lasso_pca.coef_\n",
    "names = df_principal_components.columns.to_list()\n",
    "ax = sns.barplot(x=names, y=lasso_coefs_pca)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14169bd4",
   "metadata": {},
   "source": [
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53534ca9",
   "metadata": {},
   "source": [
    "### Random Forest on original and PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "max_depth = [3, 4, 5, 6, 7]\n",
    "max_features = [\"sqrt\", \"log2\"]\n",
    "\n",
    "param_combos = []\n",
    "oob_scores = []\n",
    "oob_scores_pca = []\n",
    "\n",
    "\n",
    "for nm in n_estimators:\n",
    "    for md in max_depth:\n",
    "        for mf in max_features:\n",
    "#             print(nm,md,mf)\n",
    "            param_combos.append(f\"{nm},{md},{mf}\")\n",
    "            \n",
    "            # Build and train random forest model using base data\n",
    "            rfr = RandomForestRegressor(n_estimators=nm,\n",
    "                                       max_depth=md,\n",
    "                                       max_features=mf,\n",
    "                                       oob_score=True,\n",
    "                                       warm_start=True)\n",
    "            rfr.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Using out-of-bag (OOB) error for finding the optimal hyperparameters\n",
    "            oob_scores.append(rfr.oob_score_)\n",
    "            \n",
    "            del rfr\n",
    "            \n",
    "            # Build and train random forest model using PCA data\n",
    "            rfr_pca = RandomForestRegressor(n_estimators=nm,\n",
    "                                       max_depth=md,\n",
    "                                       max_features=mf,\n",
    "                                       oob_score=True,\n",
    "                                       warm_start=True)\n",
    "            rfr_pca.fit(X_train_pca, y_train)\n",
    "            \n",
    "            # Using out-of-bag (OOB) error for finding the optimal hyperparameters\n",
    "            oob_scores_pca.append(rfr_pca.oob_score_)\n",
    "            \n",
    "            del rfr_pca\n",
    "            \n",
    "# Obtain index for maximum OOB-score\n",
    "best_index = oob_scores.index(max(oob_scores))\n",
    "best_index_pca = oob_scores_pca.index(max(oob_scores_pca))\n",
    "\n",
    "# Obtain the best hyperparameters associated with that index\n",
    "rfr_best_nm, rfr_best_md, rfr_best_mf = param_combos[best_index].split(',')\n",
    "rfr_best_nm_pca, rfr_best_md_pca, rfr_best_mf_pca = param_combos[best_index_pca].split(',')\n",
    "\n",
    "print(param_combos[best_index].split(','))\n",
    "print(param_combos[best_index_pca].split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54086c",
   "metadata": {},
   "source": [
    "#### Training Models with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca041307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Best hyperparameters:\n",
    "['300', '7', 'sqrt']\n",
    "'''\n",
    "\n",
    "best_rfr = RandomForestRegressor(n_estimators=int(rfr_best_nm),\n",
    "                                max_depth=int(rfr_best_md),\n",
    "                                max_features=rfr_best_mf)\n",
    "best_rfr.fit(X_train_scaled, y_train)\n",
    "rfr_y_pred = best_rfr.predict(X_test_scaled)\n",
    "print('Root Mean Squared Logarithmic Error (RMSLE):', mean_squared_log_error(y_test, rfr_y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Best hyperparameters:\n",
    "['300', '3', '0.01']\n",
    "'''\n",
    "\n",
    "best_rfr_pca = RandomForestRegressor(n_estimators=int(rfr_best_nm_pca),\n",
    "                                max_depth=int(rfr_best_md_pca),\n",
    "                                max_features=rfr_best_mf_pca)\n",
    "best_rfr_pca.fit(X_train_pca, y_train)\n",
    "rfr_y_pred_pca = best_rfr_pca.predict(X_test_pca)\n",
    "print('Root Mean Squared Logarithmic Error (RMSLE):', mean_squared_log_error(y_test, rfr_y_pred_pca, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec7163",
   "metadata": {},
   "source": [
    "#### Graph Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize and sort feature importances in descending order\n",
    "rfr_tuned_importance = best_rfr.feature_importances_\n",
    "feature_importance_dict = dict(zip(df_x.columns.to_list(), rfr_tuned_importance))\n",
    "sorted_dict = dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plot the feature importance graph\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.bar(sorted_dict.keys(),sorted_dict.values())\n",
    "plt.xlabel(\"Features\",labelpad = 20)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance for Random Forest Regression')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bde6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize and sort feature importances in descending order\n",
    "rfr_tuned_importance = best_rfr.feature_importances_\n",
    "feature_importance_dict = dict(zip(df_x.columns.to_list(), rfr_tuned_importance))\n",
    "sorted_dict = dict(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "# Plot the feature importance graph\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.bar(sorted_dict.keys(),sorted_dict.values())\n",
    "plt.xlabel(\"Features\",labelpad = 20)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Top 10 Feature Importance for Random Forest Regression')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns=['PC' + str(i + 1) for i in range(n_components)]\n",
    "rfr_pca_tuned_importance = best_rfr_pca.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.bar(pca_columns,rfr_pca_tuned_importance)\n",
    "plt.xlabel(\"Features\",labelpad = 20)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance for Random Forest Classifier with PCA')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed03179",
   "metadata": {},
   "source": [
    "### XGBoost on original and PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "max_depth = [3, 4, 5, 6, 7]\n",
    "learning_rate = [0.01, 0.1, 0.5]\n",
    "\n",
    "param_combos = []\n",
    "rmsle = []\n",
    "rmsle_pca = []\n",
    "\n",
    "\n",
    "for nm in n_estimators:\n",
    "    for md in max_depth:\n",
    "        for lr in learning_rate:\n",
    "#             print(nm,md,lr)\n",
    "            param_combos.append(f\"{nm},{md},{lr}\")\n",
    "            \n",
    "            # Build XGBoost model for base data\n",
    "            xgbr = XGBRegressor(n_estimators=nm,\n",
    "                               max_depth=md,\n",
    "                               learning_rate=lr)\n",
    "            xgbr.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Using validation set and RMSLE for finding the optimal hyperparameters\n",
    "            xgbr_y_pred = xgbr.predict(X_val_scaled)\n",
    "            rmsle.append(mean_squared_log_error(y_val, xgbr_y_pred, squared=False))\n",
    "            \n",
    "            del xgbr\n",
    "\n",
    "    \n",
    "            # Build XGBoost model for PCA data\n",
    "            xgbr_pca = XGBRegressor(n_estimators=nm,\n",
    "                               max_depth=md,\n",
    "                               learning_rate=lr)\n",
    "            xgbr_pca.fit(X_train_pca, y_train)\n",
    "            \n",
    "            # Using validation set and RMSLE for finding the optimal hyperparameters\n",
    "            xgbr_y_pred_pca = xgbr_pca.predict(X_val_pca)\n",
    "            rmsle_pca.append(mean_squared_log_error(y_val, xgbr_y_pred_pca, squared=False))\n",
    "            \n",
    "            del xgbr_pca\n",
    "            \n",
    "            \n",
    "# Obtain index for minimum RMSLE\n",
    "best_index = rmsle.index(min(rmsle))\n",
    "best_index_pca = rmsle_pca.index(min(rmsle_pca))\n",
    "\n",
    "# Obtain the best hyperparameters associated with that index \n",
    "xgbr_best_nm, xgbr_best_md, xgbr_best_mf = param_combos[best_index].split(',')\n",
    "xgbr_best_nm_pca, xgbr_best_md_pca, xgbr_best_mf_pca = param_combos[best_index_pca].split(',')\n",
    "\n",
    "print(param_combos[best_index].split(','))\n",
    "print(param_combos[best_index_pca].split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddde811",
   "metadata": {},
   "source": [
    "#### Training Models with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Best hyperparameters:\n",
    "['200', '3', '0.1']\n",
    "'''\n",
    "\n",
    "best_xgbr = XGBRegressor(n_estimators=int(xgbr_best_nm),\n",
    "                       max_depth=int(xgbr_best_md),\n",
    "                       learning_rate=xgbr_best_mf)\n",
    "best_xgbr.fit(X_train_scaled, y_train)\n",
    "xgbr_best_y_pred = best_xgbr.predict(X_test_scaled)\n",
    "print('Root Mean Squared Logarithmic Error (RMSLE):', mean_squared_log_error(y_test, xgbr_best_y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5add02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Best hyperparameters:\n",
    "['300', '3', '0.01']\n",
    "'''\n",
    "\n",
    "best_xgbr_pca = XGBRegressor(n_estimators=int(xgbr_best_nm_pca),\n",
    "                       max_depth=int(xgbr_best_md_pca),\n",
    "                       learning_rate=xgbr_best_mf_pca)\n",
    "best_xgbr_pca.fit(X_train_pca, y_train)\n",
    "xgbr_best_pca_y_pred = best_xgbr_pca.predict(X_test_pca)\n",
    "print('Root Mean Squared Logarithmic Error (RMSLE):', mean_squared_log_error(y_test, xgbr_best_pca_y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df26b98",
   "metadata": {},
   "source": [
    "#### Graph Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3925fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned_importance = best_xgbr.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.bar(df_x.columns.to_list(),xgb_tuned_importance)\n",
    "plt.xlabel('Features',labelpad = 20)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance for XGBoost')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pca_tuned_importance =best_xgbr_pca.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.bar(pca_columns,xgb_pca_tuned_importance)\n",
    "plt.xlabel(\"Features\",labelpad = 20)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance for XBoost with PCA')\n",
    "plt.xticks(rotation=90)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fd70b",
   "metadata": {},
   "source": [
    "## Neural Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Root Mean Squared Logarithmic Error in TF\n",
    "def rmsle(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.log(y_pred + 1) - K.log(y_true + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09237da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN(x_train, reg_param, drop_out, lr, pca):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if pca == 0: \n",
    "        # The Input Layer :\n",
    "        model.add(Dense(78, kernel_initializer='normal',input_dim = x_train.shape[1], activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "\n",
    "        # The Hidden Layer:\n",
    "        model.add(Dense(32, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "        model.add(Dropout(drop_out))\n",
    "        \n",
    "    if pca == 1:\n",
    "        model.add(Dense(36, kernel_initializer='normal',input_dim = x_train.shape[1], activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "\n",
    "    # Hidden Layers\n",
    "    model.add(Dense(16, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(8, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(4, kernel_initializer='normal',activation='relu', kernel_regularizer=regularizers.l1(reg_param)))\n",
    "\n",
    "    # The Output Layer :\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "    \n",
    "    #Define Learning Rate of Optimizer\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    #Compile Model\n",
    "    model.compile(optimizer=optimizer, loss=rmsle, metrics=[rmsle])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d008ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example structure\n",
    "model_example = model = build_NN(X_train, 0, 0, 0, pca=0)\n",
    "model_example.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes a long time to run and sometimes yields inconsistent results/errors\n",
    "# In the next cell are the tuned parameters used for results shown in the report\n",
    "# They are a combination of results from this grid search tuning approach and manual\n",
    "# trial and error\n",
    "\n",
    "# #Hyperparameters\n",
    "# reg_param = [0, 0.01, 0.001, 0.0001]\n",
    "# drop_outs = [0, 0.1, 0.2, 0.3]\n",
    "# lrs = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "# epochs = [10, 20, 50, 100]\n",
    "\n",
    "# best_rmsle = 100\n",
    "# best_rmsle_pca = 100\n",
    "\n",
    "# for rp in reg_param:\n",
    "#     for drop_out in drop_outs:\n",
    "#         for lr in lrs:\n",
    "#             for epoch in epochs:\n",
    "\n",
    "#                 #Model without PCA\n",
    "                \n",
    "#                 #Create Model\n",
    "#                 model = build_NN(X_train, rp, drop_out, lr, 0)\n",
    "\n",
    "#                 # Train model\n",
    "#                 model.fit(X_train, y_train, epochs=epoch, verbose=0)\n",
    "\n",
    "#                 # Evaluate model\n",
    "#                 val_loss, val_rmsle = model.evaluate(X_val, y_val)\n",
    "\n",
    "#                 if val_rmsle < best_rmsle:\n",
    "\n",
    "#                     best_reg = rp\n",
    "#                     best_do = drop_out\n",
    "#                     best_lr = lr\n",
    "#                     best_epoch = epoch\n",
    "\n",
    "#                     best_rmsle = val_rmsle\n",
    "\n",
    "#                 del model\n",
    "#                 del val_rmsle\n",
    "\n",
    "#                 #Model with PCA\n",
    "                \n",
    "#                 #Create Model\n",
    "#                 model = build_NN(X_train_pca, rp, drop_out, lr, 1)\n",
    "\n",
    "#                 # Train your model\n",
    "#                 model.fit(X_train_pca, y_train, epochs=epoch, verbose=0)\n",
    "\n",
    "#                 # Evaluate your model using RMSLE\n",
    "#                 val_loss, val_rmsle = model.evaluate(X_val_pca, y_val)\n",
    "\n",
    "#                 if val_rmsle < best_rmsle_pca:\n",
    "                    \n",
    "#                     best_reg_pca = rp\n",
    "#                     best_do_pca = drop_out\n",
    "#                     best_lr_pca = lr\n",
    "#                     best_epoch = epoch\n",
    "\n",
    "#                     best_rmsle_pca = val_rmsle\n",
    "\n",
    "#                 del model\n",
    "#                 del val_rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters for Hyperparameter Tuning (model without PCA)\n",
    "reg_param = 0.0001\n",
    "drop_out = 0.1\n",
    "learning_rate = 0.01\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf012ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = build_NN(X_train, reg_param, drop_out, learning_rate, 0)\n",
    "\n",
    "# Train your model and store the history object\n",
    "history = model1.fit(X_train, y_train, epochs=epoch, verbose=0, validation_data=(X_val, y_val))\n",
    "\n",
    "# Get the training and validation loss from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss as a function of epoch\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Loss Plots for Data Without PCA')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_rmsle = model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eff760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters for Hyperparameter Tuning (model with PCA)\n",
    "reg_param = 0.01\n",
    "drop_out = 0.2\n",
    "learning_rate = 0.01\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a81b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_NN(X_train_pca, reg_param, drop_out, learning_rate, 1)\n",
    "\n",
    "# Train your model and store the history object\n",
    "history = model2.fit(X_train_pca, y_train, epochs=epoch, verbose=0, validation_data=(X_val_pca, y_val))\n",
    "\n",
    "# Get the training and validation loss from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss as a function of epoch\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Loss Plots for Data Without PCA')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_rmsle = model2.evaluate(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b25b93",
   "metadata": {},
   "source": [
    "## Group-Specific "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#Gaussian Mixture Modelling\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm.fit(X_train_scaled)\n",
    "\n",
    "# Predict the labels for the data\n",
    "labels = gmm.predict(X_train_scaled)\n",
    "\n",
    "# Create a list of y values for each cluster\n",
    "y_cluster = [y_train[labels == i] for i in range(gmm.n_components)]\n",
    "\n",
    "# Sort clusters in ascending order (by median)\n",
    "sorted_y_cluster = sorted(y_cluster, key=lambda x: np.median(x))\n",
    "\n",
    "# Boxplot of clusters\n",
    "plt.boxplot(sorted_y_cluster)\n",
    "plt.title(\"Boxplots of House Prices by Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"House Price\")\n",
    "plt.show()\n",
    "\n",
    "# Count number of points in each cluster\n",
    "for i in range(0,len(sorted_y_cluster)):\n",
    "    print(\"Cluster \" + str(i+1) + \": \" + str(len(sorted_y_cluster[i])) + \" data points\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a list to store the RMSLE for each cluster-specific model\n",
    "rmsle_list = []\n",
    "\n",
    "# Loop through each cluster\n",
    "for i in range(gmm.n_components):\n",
    "    # Get the indices of the samples belonging to the current cluster\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "    # Split the data into training and testing sets for the current cluster\n",
    "    X_cluster_train, X_cluster_test, y_cluster_train, y_cluster_test = train_test_split(\n",
    "        X_train.iloc[cluster_indices], y_train.iloc[cluster_indices], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a linear regression model for the current cluster\n",
    "    linear_cluster = LinearRegression()\n",
    "    linear_cluster.fit(X_cluster_train, y_cluster_train)\n",
    "\n",
    "    # Calculate the RMSLE for the current cluster-specific model\n",
    "    y_pred = linear_cluster.predict(X_cluster_test)\n",
    "    y_pred_clipped = np.clip(y_pred, 0, None)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_cluster_test, y_pred_clipped))\n",
    "\n",
    "    rmsle_list.append(rmsle)\n",
    "    print(f\"Cluster {i+1} RMSLE: {rmsle}\")\n",
    "\n",
    "# Calculate the average RMSLE across all cluster-specific models\n",
    "average_rmsle = np.mean(rmsle_list)\n",
    "print(f\"Average RMSLE for cluster-specific models: {average_rmsle}\")\n",
    "\n",
    "# Compare the average RMSLE with the baseline RMSLE\n",
    "baseline_rmsle = np.sqrt(mean_squared_log_error(y_test, linear.predict(X_test)))\n",
    "print(f\"Baseline RMSLE: {baseline_rmsle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a2f7e",
   "metadata": {},
   "source": [
    "Based on the results you provided, the cluster-specific linear regression models have a higher average RMSLE (0.5308) compared to the baseline linear regression model (0.1915). This indicates that the group-specific regression for each cluster found in the GMM does not enhance the performance compared to the linear regression on the original data in this case.\n",
    "\n",
    "One possible reason for this outcome is that the GMM clustering is not effectively capturing the underlying structure of the data in a way that benefits the prediction of house prices. Another reason could be the presence of a cluster with a particularly high RMSLE (Cluster 4) which is skewing the average RMSLE upwards. You could try experimenting with different clustering techniques or varying the number of clusters to see if you can obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the RMSLE for each cluster-specific model\n",
    "rmsle_list = []\n",
    "\n",
    "# Loop through each cluster, excluding Cluster 4\n",
    "for i in range(gmm.n_components - 1):  # Change the loop range to exclude the last cluster\n",
    "    # Get the indices of the samples belonging to the current cluster\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "    # Split the data into training and testing sets for the current cluster\n",
    "    X_cluster_train, X_cluster_test, y_cluster_train, y_cluster_test = train_test_split(\n",
    "        X_train.iloc[cluster_indices], y_train.iloc[cluster_indices], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a linear regression model for the current cluster\n",
    "    linear_cluster = LinearRegression()\n",
    "    linear_cluster.fit(X_cluster_train, y_cluster_train)\n",
    "\n",
    "    # Calculate the RMSLE for the current cluster-specific model\n",
    "    y_pred = linear_cluster.predict(X_cluster_test)\n",
    "    y_pred_clipped = np.clip(y_pred, 0, None)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_cluster_test, y_pred_clipped))\n",
    "\n",
    "    rmsle_list.append(rmsle)\n",
    "    print(f\"Cluster {i+1} RMSLE: {rmsle}\")\n",
    "\n",
    "# Calculate the average RMSLE across all cluster-specific models, excluding Cluster 4\n",
    "average_rmsle_without_cluster4 = np.mean(rmsle_list)\n",
    "print(f\"Average RMSLE for cluster-specific models without Cluster 4: {average_rmsle_without_cluster4}\")\n",
    "\n",
    "# Compare the average RMSLE without Cluster 4 with the baseline RMSLE\n",
    "print(f\"Baseline RMSLE: {baseline_rmsle}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884809d2",
   "metadata": {},
   "source": [
    "Based on the updated results without Cluster 4, the average RMSLE for the cluster-specific linear regression models is 0.2205, which is still higher than the baseline RMSLE of 0.1915. This indicates that even without considering Cluster 4, the group-specific regression for each cluster found in the GMM does not enhance the performance compared to the linear regression on the original data.\n",
    "\n",
    "This could be because the GMM clustering is still not effectively capturing the underlying structure of the data in a way that benefits the prediction of house prices. It may be worth exploring other clustering techniques, varying the number of clusters, or even trying different regression algorithms to see if you can obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b82764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_log_error, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define clustering algorithms with their specific parameters\n",
    "clustering_algorithms = {\n",
    "    'KMeans': {'algorithm': KMeans, 'params': {'n_clusters': None, 'random_state': 42}},\n",
    "    'AgglomerativeClustering': {'algorithm': AgglomerativeClustering, 'params': {'n_clusters': None}},\n",
    "    'GaussianMixture': {'algorithm': GaussianMixture, 'params': {'n_components': None, 'random_state': 42}}\n",
    "}\n",
    "\n",
    "# Define regression algorithms\n",
    "regression_algorithms = {\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'LASSO': Lasso,\n",
    "    'RandomForestRegressor': RandomForestRegressor,\n",
    "    'XGBoost': XGBRegressor,\n",
    "    'NeuralNetworkRegressor': MLPRegressor\n",
    "}\n",
    "\n",
    "# Define the range of the number of clusters to explore\n",
    "n_clusters_range = range(2, 8)\n",
    "\n",
    "# Initialize variables to store the best model information\n",
    "best_rmsle = float('inf')\n",
    "best_clustering = None\n",
    "best_n_clusters = None\n",
    "best_regression = None\n",
    "\n",
    "# Loop through each clustering algorithm\n",
    "for clustering_name, clustering_info in clustering_algorithms.items():\n",
    "    # Loop through the range of the number of clusters\n",
    "    for n_clusters in n_clusters_range:\n",
    "        # Set the appropriate parameter for the number of clusters or components\n",
    "        if clustering_name == 'GaussianMixture':\n",
    "            clustering_info['params']['n_components'] = n_clusters\n",
    "        else:\n",
    "            clustering_info['params']['n_clusters'] = n_clusters\n",
    "\n",
    "        # Initialize the clustering algorithm with the current number of clusters and specific parameters\n",
    "        clustering = clustering_info['algorithm'](**clustering_info['params'])\n",
    "\n",
    "        # Fit the clustering algorithm to the training data and predict the labels\n",
    "        if clustering_name == 'GaussianMixture':\n",
    "            clustering.fit(X_train_scaled)\n",
    "            labels = clustering.predict(X_train_scaled)\n",
    "        else:\n",
    "            labels = clustering.fit_predict(X_train_scaled)\n",
    "        \n",
    "        # Calculate the silhouette score for the current clustering\n",
    "        silhouette_avg = silhouette_score(X_train_scaled, labels)\n",
    "        \n",
    "        # Loop through each regression algorithm\n",
    "        for regression_name, Regression in regression_algorithms.items():\n",
    "            # Initialize a list to store the RMSLE for each cluster-specific model\n",
    "            rmsle_list = []\n",
    "            \n",
    "            # Loop through each cluster\n",
    "            for i in range(n_clusters):\n",
    "                # Get the indices of the samples belonging to the current cluster\n",
    "                cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "                # Split the data into training and testing sets for the current cluster\n",
    "                X_cluster_train, X_cluster_test, y_cluster_train, y_cluster_test = train_test_split(\n",
    "                    X_train.iloc[cluster_indices], y_train.iloc[cluster_indices], test_size=0.2, random_state=42)\n",
    "\n",
    "                # Train a regression model for the current cluster\n",
    "                regression = Regression()\n",
    "                regression.fit(X_cluster_train, y_cluster_train)\n",
    "\n",
    "                # Calculate the RMSLE for the current cluster-specific model\n",
    "                y_pred = regression.predict(X_cluster_test)\n",
    "                y_pred_clipped = np.clip(y_pred, 0, None)\n",
    "                rmsle = np.sqrt(mean_squared_log_error(y_cluster_test, y_pred_clipped))\n",
    "\n",
    "                rmsle_list.append(rmsle)\n",
    "\n",
    "            # Calculate the average RMSLE across all cluster-specific models\n",
    "            average_rmsle = np.mean(rmsle_list)\n",
    "            \n",
    "            # Update the best model information if the current combination has a lower average RMSLE\n",
    "            if average_rmsle < best_rmsle:\n",
    "                best_rmsle = average_rmsle\n",
    "                best_clustering = clustering_name\n",
    "                best_n_clusters = n_clusters\n",
    "                best_regression = regression_name\n",
    "\n",
    "            print(f\"{clustering_name} with {n_clusters} clusters and {regression_name}: Average RMSLE = {average_rmsle:.4f}, Silhouette Score = {silhouette_avg:.4f}\")\n",
    "\n",
    "# Print the best model information\n",
    "print(f\"\\nBest combination: {best_clustering} with {best_n_clusters} clusters and {best_regression}\")\n",
    "print(f\"Best average RMSLE: {best_rmsle:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "426.375px",
    "left": "762.825px",
    "right": "20px",
    "top": "45px",
    "width": "617.05px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
